{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_int, truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def to_pil(out):\n",
    "    img_np = out.detach().cpu().numpy()[0]\n",
    "    img_np = np.clip((img_np + 1) / 2.0 * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # C X H X W to H x W x C\n",
    "    img_np = img_np.transpose(1, 2, 0)\n",
    "\n",
    "    return Image.fromarray(img_np)\n",
    "\n",
    "def show(img):\n",
    "    pred = np.clip((img.detach().cpu().squeeze().numpy()+1) / 2.0 * 255, 0, 255)\n",
    "    pred = pred.transpose(1, 2, 0).astype(\"uint8\")\n",
    "    pred = Image.fromarray(pred)\n",
    "    display(pred)\n",
    "    \n",
    "truncation = 0.4  # truncation trick (0-1, larger value will generate more diverse image but the quality is worse)\n",
    "class_vector = one_hot_from_int(250)  # input imagenet label here (husky)\n",
    "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=1)\n",
    "\n",
    "noise_vector = torch.from_numpy(noise_vector)\n",
    "class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "noise_vector = noise_vector.to('cuda')\n",
    "class_vector = class_vector.to('cuda')\n",
    "model = BigGAN.from_pretrained('biggan-deep-128').cuda()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dip(img, G, z_, class_, truncation, lr, num_iter=5000):\n",
    "    \"\"\"DIP Style Reconstruction. We don't change the latent code z now, instead we change the generator's weight\"\"\"\n",
    "    G.train()\n",
    "    \n",
    "    dist = torch.nn.MSELoss()\n",
    "\n",
    "    # RMSProp or ADAM or SGD\n",
    "    optZ = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        img_pred = G(z_, class_, truncation)\n",
    "        loss = dist(img_pred, img)\n",
    "    \n",
    "        # scheduler.step(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            print(\"[Iter {}] error: {}\"\n",
    "                  .format(i, loss.item()))\n",
    "            show(img_pred)\n",
    "            show(img)\n",
    "            \n",
    "\n",
    "        optZ.zero_grad()\n",
    "        loss.backward()\n",
    "        optZ.step()\n",
    "\n",
    "        # truncate\n",
    "        # z_ = torch.clamp(z_, max=1., min=-1.)\n",
    "        \n",
    "    return G(z_, class_, truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image so that the image is in the range\n",
    "with torch.no_grad():\n",
    "    img_gt = model(noise_vector, class_vector, truncation)\n",
    "show(img_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS_256 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "TRANSFORMS_128 = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset model for different image as we will change model's weight\n",
    "model = BigGAN.from_pretrained('biggan-deep-128').cuda()  \n",
    "z_dip = torch.from_numpy(truncated_noise_sample(truncation=truncation, batch_size=1)).to('cuda')\n",
    "\n",
    "dip(img_gt, model, z_dip, class_vector, truncation, lr=1e-5, num_iter=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a real image\n",
    "img_real = Image.open(\"./data/tiger_cat.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(img_real)\n",
    "img_real = TRANSFORMS_256(img_real).cuda().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset model for different image as we will change model's weight\n",
    "# You can see that dip style reconstruction with BigGan weights is quite good for inversion/denoising\n",
    "model = BigGAN.from_pretrained('biggan-deep-256').cuda()  \n",
    "z_dip = torch.from_numpy(truncated_noise_sample(truncation=truncation, batch_size=1)).to('cuda')\n",
    "\n",
    "class_vector = one_hot_from_int(282)  # tiger-cat: 282, husky: 248\n",
    "class_vector = torch.from_numpy(class_vector).cuda()\n",
    "\n",
    "dip(img_real, model, z_dip, class_vector, truncation, lr=1e-5, num_iter=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
